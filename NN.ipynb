{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fsveFS7ikyu"
      },
      "source": [
        "# Klassifikation mit einem Neuronalen Netz\n",
        "\n",
        "## Trainingsdaten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bWQuIIshiky6"
      },
      "outputs": [],
      "source": [
        "# %pip install torch\n",
        "# %pip install torchvision\n",
        "# %pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "jux-RX43iky6"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries/modules\n",
        "from torchvision import datasets, transforms  # Import datasets and transforms from the torchvision library\n",
        "from torch.utils.data import TensorDataset, DataLoader  # Import TensorDataset and DataLoader from torch.utils.data\n",
        "import torch  # Import the main PyTorch library\n",
        "import torch.nn as nn  # Import the neural network module from PyTorch\n",
        "import torch.nn.functional as F  # Import functional components of neural networks from PyTorch\n",
        "import torch.optim as optim  # Import optimization algorithms from PyTorch\n",
        "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting\n",
        "import numpy as np  # Import the NumPy library for numerical operations\n",
        "\n",
        "# Define a function 'load' that loads data from a given file\n",
        "def load(data):\n",
        "    return np.load(data)['arr_0']  # Load data from the specified file using NumPy and return it\n",
        "\n",
        "# Load the training and testing data and labels\n",
        "train_data = load('Data\\K49-data\\k49-train-imgs.npz')  # Load training image data from a file\n",
        "test_data = load('Data\\K49-data\\k49-test-imgs.npz')    # Load testing image data from a file\n",
        "\n",
        "train_labels = load('Data\\K49-data\\k49-train-labels.npz')  # Load training labels from a file\n",
        "test_labels = load('Data\\K49-data\\k49-test-labels.npz')    # Load testing labels from a file\n",
        "# Convert the data to the appropriate data type (float32)\n",
        "train_data = train_data.astype(np.float32)\n",
        "test_data = test_data.astype(np.float32)\n",
        "\n",
        "# Combine the training data and labels into a PyTorch TensorDataset\n",
        "train_dataset = TensorDataset(torch.tensor(train_data), torch.tensor(train_labels))\n",
        "test_dataset = TensorDataset(torch.tensor(test_data), torch.tensor(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the csv module for working with CSV files\n",
        "import csv\n",
        "\n",
        "# Define a list of encodings to try\n",
        "encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
        "\n",
        "# Define a function to load the mapping from the CSV file with various encodings\n",
        "def load_class_mapping(csv_file):\n",
        "    class_mapping = None  # Initialize class_mapping as None\n",
        "\n",
        "    for encoding in encodings_to_try:\n",
        "        try:\n",
        "            class_mapping = {}  # Initialize an empty dictionary to store the mapping\n",
        "\n",
        "            # Open the CSV file specified by csv_file for reading with the current encoding\n",
        "            with open(csv_file, newline='', encoding=encoding) as csvfile:\n",
        "                # Create a CSV reader object that treats the first row as headers and maps columns to keys\n",
        "                csvreader = csv.DictReader(csvfile)\n",
        "\n",
        "                # Iterate through each row (record) in the CSV file\n",
        "                for row in csvreader:\n",
        "                    # Extract the 'index' value from the current row and convert it to an integer\n",
        "                    index = int(row['index'])\n",
        "\n",
        "                    # Extract the 'char' value from the current row\n",
        "                    char = row['char']\n",
        "\n",
        "                    # Add an entry to the class_mapping dictionary, mapping index to char\n",
        "                    class_mapping[index] = char\n",
        "\n",
        "            # If successful, break out of the loop\n",
        "            break\n",
        "        except UnicodeDecodeError:\n",
        "            # If there's a decoding error, try the next encoding in the list\n",
        "            pass\n",
        "\n",
        "    # Check if class_mapping is still None (indicating no successful decoding)\n",
        "    if class_mapping is None:\n",
        "        raise ValueError(\"Unable to decode the CSV file with any of the specified encodings.\")\n",
        "\n",
        "    # Return the populated class_mapping dictionary containing the mapping from index to char\n",
        "    return class_mapping\n",
        "\n",
        "# Specify the path to your CSV file (e.g., \"k49_classmap.csv\")\n",
        "csv_file_path = 'k49_classmap.csv'\n",
        "\n",
        "# Load the class mapping from the specified CSV file by calling the load_class_mapping function\n",
        "class_mapping = load_class_mapping(csv_file_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class index 23 corresponds to character: „Å≠\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the class index you want to look up\n",
        "index = 1\n",
        "class_index = test_labels[index]\n",
        "\n",
        "# Display the image using Matplotlib\n",
        "plt.imshow(test_data[index], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# Check if the class index exists in the class_mapping dictionary\n",
        "if class_index in class_mapping:\n",
        "    # If it exists, retrieve the corresponding character label\n",
        "    character = class_mapping[class_index]\n",
        "\n",
        "    # Print the result\n",
        "    print(f\"Class index {class_index} corresponds to character: {character}\")\n",
        "else:\n",
        "    # If it doesn't exist, print a message indicating that it was not found\n",
        "    print(f\"Class index {class_index} not found in the mapping.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkdXQsEAiky8"
      },
      "source": [
        "## Definition des Neuronalen Netzes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YNrh6L-Piky8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Set parameters for the feedforward neural network\n",
        "input_size = 784  # Update the input size to 784 (28*28)\n",
        "hidden_size = 2048*2  # Number of neurons in the hidden layer\n",
        "num_classes = 49    # Number of output classes (categories)\n",
        "num_epochs = 20    # Number of training epochs\n",
        "batch_size = 2048   # Batch size for data processing\n",
        "learning_rate = 0.0005  # Learning rate for the optimizer\n",
        "\n",
        "# Create data loaders for training and testing data\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the architecture of the neural network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Define the input layer that accepts 784-dimensional input\n",
        "        self.input_layer = nn.Linear(input_size, input_size).to(dtype=torch.float32)\n",
        "        self.l1 = nn.Linear(input_size, hidden_size).to(dtype=torch.float32)  # Fully connected layer\n",
        "        self.relu = nn.ReLU()  # Rectified Linear Unit (ReLU) activation function\n",
        "        # Define the output layer\n",
        "        self.l2 = nn.Linear(hidden_size, num_classes).to(dtype=torch.float32)  # Fully connected layer\n",
        "\n",
        "    def forward(self, x):  # Forward pass through the network\n",
        "        x = self.input_layer(x)  # Pass input through the input layer\n",
        "        out = self.l1(x)  # Pass input through the first hidden layer\n",
        "        out = self.relu(out)  # Apply ReLU activation\n",
        "        out = self.l2(out)  # Pass through the output layer to get predictions\n",
        "        return out\n",
        "\n",
        "# Create an instance of the neural network model\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITU5062Viky9"
      },
      "source": [
        "## Training des Neuronalen Netz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEKoVx03iky-",
        "outputId": "2964e996-7956-4207-f528-fc87a58baf41"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Sebastian\\Desktop\\Project\\NN.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sebastian/Desktop/Project/NN.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients from previous steps\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sebastian/Desktop/Project/NN.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# Perform backpropagation to compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sebastian/Desktop/Project/NN.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()  \u001b[39m# Update model parameters using the computed gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sebastian/Desktop/Project/NN.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Print training statistics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sebastian/Desktop/Project/NN.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32mc:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[1;32mc:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define the loss function (CrossEntropyLoss) and the optimizer (Adam)\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function for classification problems\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer with specified learning rate\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)  # Total number of batches in the training data\n",
        "for epoch in range(num_epochs):  # Loop through each training epoch\n",
        "    for i, (train_data, train_labels) in enumerate(train_loader):  # Loop through each batch of data\n",
        "        # Reshape the input images to have the shape (batch_size, input_size)\n",
        "        train_data = train_data.reshape(-1, 28*28)\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(train_data)\n",
        "\n",
        "        # Calculate the loss using the defined criterion (CrossEntropyLoss)\n",
        "        loss = criterion(outputs, train_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear gradients from previous steps\n",
        "        loss.backward()  # Perform backpropagation to compute gradients\n",
        "        optimizer.step()  # Update model parameters using the computed gradients\n",
        "\n",
        "        # Print training statistics\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After training the model, save it to a file\n",
        "torch.save(model.state_dict(), 'mymodel.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04AChXxZiky-"
      },
      "source": [
        "## Test des Neuronalen Netzes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T4jfUH-iky-",
        "outputId": "1bae4f1a-e454-4d04-a663-e0fea0b6019f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced Accuracy of the model on test images: 80.96 %\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists to store predictions and ground truth labels\n",
        "p_test = []  # Model predictions of class index\n",
        "y_test = []  # Ground truth class indices\n",
        "\n",
        "# Loop through the test data in batches\n",
        "for images, labels in test_loader:\n",
        "    images = images.reshape(-1, 28*28)  # Reshape input images\n",
        "    images = images.to(dtype=torch.float32)  # Cast images to the correct data type (torch.float32)\n",
        "    outputs = model(images)  # Forward pass to get model predictions\n",
        "    _, predicted = torch.max(outputs.data, 1)  # Get the predicted class for each sample\n",
        "\n",
        "    # Append model predictions and ground truth labels to lists\n",
        "    p_test.extend(predicted.tolist())\n",
        "    y_test.extend(labels.tolist())\n",
        "\n",
        "# Calculate balanced accuracy using the provided implementation\n",
        "accs = []\n",
        "\n",
        "# Loop through each class (in this case, there are 49 classes)\n",
        "for cls in range(49):\n",
        "    # Create a mask to select samples of the current class in the ground truth\n",
        "    mask = (torch.tensor(y_test) == cls)\n",
        "    \n",
        "    # Calculate the accuracy for samples of the current class\n",
        "    cls_acc = (torch.tensor(p_test) == cls)[mask].float().mean()  # Accuracy for rows of class cls\n",
        "    \n",
        "    # Append the class accuracy to the list of accuracies\n",
        "    accs.append(cls_acc)\n",
        "\n",
        "# Calculate the final balanced accuracy by taking the mean of class accuracies\n",
        "balanced_accuracy = torch.tensor(accs).mean().item()\n",
        "\n",
        "# Print the balanced accuracy as a percentage\n",
        "print('Balanced Accuracy of the model on test images: {:.2f} %'.format(100 * balanced_accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[239. 114.  43.  39.  24.  11.  12.  12.  12.  12.  12.  12.  12.  12.\n",
            "   12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.]\n",
            " [233. 109.  18.  22.  19.  12.  13.  13.  13.  13.  13.  13.  13.  13.\n",
            "   13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.]\n",
            " [242. 112.   0.   3.   3.   4.   4.   4.   4.   4.   4.   4.   4.   4.\n",
            "    4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.]\n",
            " [243. 113.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [236.  94.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [241. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [237. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.  23. 154. 105.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [208.  89.   0.   0.   0.   0.   0.   0.   0.   0.   0.  97. 252. 240.\n",
            "   89.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.]\n",
            " [232. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.  13. 174. 255.\n",
            "  238.  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 171.]\n",
            " [241. 105.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  19. 185.\n",
            "  255. 225.  72.   0.   0.   0.   0.   0.   0.   0.   0.  44. 190. 255.]\n",
            " [238.  98.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  25.\n",
            "  192. 249. 243.  90.   0.   0.   0.   0.   0.   0.  64. 226. 245. 215.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   21. 188. 255. 248.  88.   0.   0.   0.   0.  80. 223. 255. 206.  46.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.  18. 189. 255. 245. 104.   0.   0. 102. 239. 234. 187.  35.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.  16. 173. 255. 241. 115. 100. 243. 255. 158.  11.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.  11. 152. 243. 253. 236. 246. 158.  11.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   1. 129. 232. 234. 175.   9.   0.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.  84. 221. 206.  31.   0.   0.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.  12. 185. 226.  92.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.  78. 219. 184.  10.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   1. 158. 223.  98.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.  30. 203. 202.  30.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.  90. 224. 177.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in Tkinter callback\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
            "    return self.func(*args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sebastian\\AppData\\Local\\Temp\\ipykernel_9744\\3026150142.py\", line 134, in classify_handwriting\n",
            "    outputs = model(img_tensor)  # Forward pass to get model predictions\n",
            "              ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sebastian\\AppData\\Local\\Temp\\ipykernel_9744\\3925496919.py\", line 38, in forward\n",
            "    x = self.input_layer(x)  # Pass input through the input layer\n",
            "        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x784)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[239. 114.  43.  39.  24.  11.  12.  12.  12.  12.  12.  12.  12.  12.\n",
            "   12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.  12.]\n",
            " [233. 109.  18.  22.  19.  12.  13.  13.  13.  13.  13.  13.  13.  13.\n",
            "   13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.]\n",
            " [242. 112.   0.   3.   3.   4.   4.   4.   4.   4.   4.   4.   4.   4.\n",
            "    4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.]\n",
            " [243. 113.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [236.  94.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [241. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [237. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.  23. 154. 105.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [208.  89.   0.   0.   0.   0.   0.   0.   0.   0.   0.  97. 252. 240.\n",
            "   89.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.]\n",
            " [232. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.  13. 174. 255.\n",
            "  238.  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 171.]\n",
            " [241. 105.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  19. 185.\n",
            "  255. 225.  72.   0.   0.   0.   0.   0.   0.   0.   0.  44. 190. 255.]\n",
            " [238.  98.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  25.\n",
            "  192. 249. 243.  90.   0.   0.   0.   0.   0.   0.  64. 226. 245. 215.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   21. 188. 255. 248.  88.   0.   0.   0.   0.  80. 223. 255. 206.  46.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.  18. 189. 255. 245. 104.   0.   0. 102. 239. 234. 187.  35.   0.]\n",
            " [243. 114.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.  16. 173. 255. 241. 115. 100. 243. 255. 158.  11.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.  11. 152. 243. 253. 236. 246. 158.  11.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   1. 129. 232. 234. 175.   9.   0.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.  84. 221. 206.  31.   0.   0.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.  12. 185. 226.  92.   0.   0.   0.   0.   0.   0.]\n",
            " [243. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.  78. 219. 184.  10.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   1. 158. 223.  98.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.  30. 203. 202.  30.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [244. 115.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.  90. 224. 177.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in Tkinter callback\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
            "    return self.func(*args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sebastian\\AppData\\Local\\Temp\\ipykernel_9744\\3026150142.py\", line 134, in classify_handwriting\n",
            "    outputs = model(img_tensor)  # Forward pass to get model predictions\n",
            "              ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sebastian\\AppData\\Local\\Temp\\ipykernel_9744\\3925496919.py\", line 38, in forward\n",
            "    x = self.input_layer(x)  # Pass input through the input layer\n",
            "        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x784)\n"
          ]
        }
      ],
      "source": [
        "import tkinter as tk  # Import the tkinter library for creating the GUI\n",
        "import win32gui  # Import win32gui for interacting with the Windows GUI\n",
        "from PIL import ImageGrab, Image  # Import the PIL library for image processing\n",
        "import csv  # Import the CSV library for reading CSV files\n",
        "\n",
        "# Define a function to load the class mapping from a CSV file with various encodings\n",
        "def load_class_mapping(csv_file):\n",
        "    # Define a list of encodings to try when reading the CSV file\n",
        "    encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
        "    \n",
        "    # Iterate through the list of encodings and attempt to read the CSV file\n",
        "    for encoding in encodings_to_try:\n",
        "        try:\n",
        "            # Initialize an empty dictionary to store the class mapping\n",
        "            class_mapping = {}   \n",
        "            # Open the CSV file with the specified encoding\n",
        "\n",
        "            with open(csv_file, newline='', encoding=encoding) as csvfile:\n",
        "                csvreader = csv.DictReader(csvfile)          \n",
        "                # Iterate through each row in the CSV file\n",
        "\n",
        "                for row in csvreader:\n",
        "                    # Extract the 'index' and 'char' values from the row\n",
        "                    index = int(row['index'])\n",
        "                    char = row['char']                  \n",
        "                    # Add the mapping of 'index' to 'char' in the class_mapping dictionary\n",
        "                    class_mapping[index] = char\n",
        "            # Return the class mapping if successful\n",
        "\n",
        "            return class_mapping\n",
        "        # Handle Unicode decoding errors\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            continue   \n",
        "    # Raise an exception if unable to decode the CSV file with any of the specified encodings\n",
        "    raise ValueError(\"Unable to decode the CSV file with any of the specified encodings.\")\n",
        "\n",
        "# Specify the path to your CSV file (e.g., \"k49_classmap.csv\")\n",
        "csv_file_path = 'k49_classmap.csv'\n",
        "\n",
        "# Load the class mapping from the CSV file using the function\n",
        "class_mapping = load_class_mapping(csv_file_path)\n",
        "\n",
        "# Define your neural network model and load pre-trained weights\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)  # Create an instance of the neural network\n",
        "model.load_state_dict(torch.load('mymodel.pth'))  # Load pre-trained weights\n",
        "model.eval()  # Set the model to evaluation mode (no training)\n",
        "\n",
        "# Create a tkinter GUI application\n",
        "class App(tk.Tk):\n",
        "    def __init__(self):\n",
        "        tk.Tk.__init__(self)  # Initialize the tkinter application\n",
        "        self.x = self.y = 0  # Initialize mouse coordinates\n",
        "        self.drawing_area = tk.Canvas(self, width=28*20, height=28*20, bg=\"white\", cursor=\"cross\")  # Create a canvas for drawing\n",
        "        self.result_label = tk.Label(self, font=(\"Helvetica\", 24))  # Create a label for displaying recognized character\n",
        "        self.classify_btn = tk.Button(self, text=\"Recognize\", command=self.classify_handwriting)  # Create a button for recognition\n",
        "        self.button_clear = tk.Button(self, text=\"Clear\", command=self.clear_all)  # Create a button to clear the canvas\n",
        "        \n",
        "        # Variables to track drawing\n",
        "        self.drawing = False  # Initialize drawing state\n",
        "        self.last_x = 0  # Initialize last mouse X coordinate\n",
        "        self.last_y = 0  # Initialize last mouse Y coordinate\n",
        "\n",
        "        # Bind mouse events to functions\n",
        "        self.drawing_area.bind(\"<Button-1>\", self.start_drawing)  # Bind left mouse button press to start_drawing function\n",
        "        self.drawing_area.bind(\"<B1-Motion>\", self.draw_lines)  # Bind mouse motion to draw_lines function\n",
        "        self.drawing_area.bind(\"<ButtonRelease-1>\", self.stop_drawing)  # Bind left mouse button release to stop_drawing function\n",
        "\n",
        "        # Grid structure for placing widgets\n",
        "        self.drawing_area.grid(row=0, column=0, pady=2, sticky=tk.W)  # Place the canvas in the grid\n",
        "        self.result_label.grid(row=0, column=1, pady=2, padx=2)  # Place the label in the grid\n",
        "        self.classify_btn.grid(row=1, column=0, pady=2)  # Place the classify button in the grid\n",
        "        self.button_clear.grid(row=2, column=0, pady=2)  # Place the clear button in the grid\n",
        "\n",
        "    def start_drawing(self, event):\n",
        "        self.drawing = True  # Set drawing state to True\n",
        "        self.last_x = event.x  # Record the current mouse X coordinate\n",
        "        self.last_y = event.y  # Record the current mouse Y coordinate\n",
        "    def draw_lines(self, event):\n",
        "        if self.drawing:\n",
        "            x, y = event.x, event.y  # Get the current mouse coordinates\n",
        "            # Calculate the number of intermediate points to add\n",
        "            num_points = max(abs(x - self.last_x), abs(y - self.last_y))\n",
        "            \n",
        "            if num_points > 0:\n",
        "                # Calculate the step size for the interpolation\n",
        "                x_step = (x - self.last_x) / num_points\n",
        "                y_step = (y - self.last_y) / num_points\n",
        "                \n",
        "                for _ in range(int(num_points)):  # Convert num_points to an integer\n",
        "                    # Create a line from the last recorded coordinates to an intermediate point\n",
        "                    self.drawing_area.create_line(\n",
        "                        (self.last_x, self.last_y, self.last_x + x_step, self.last_y + y_step),\n",
        "                        fill=\"black\",\n",
        "                        width=30\n",
        "                    )\n",
        "                    \n",
        "                    # Update the last coordinates for the next iteration\n",
        "                    self.last_x += x_step\n",
        "                    self.last_y += y_step\n",
        "\n",
        "    def stop_drawing(self, event):\n",
        "        self.drawing = False  # Set drawing state to False\n",
        "\n",
        "    def clear_all(self):\n",
        "        self.drawing_area.delete(\"all\")  # Clear the canvas\n",
        "        self.result_label.configure(text=\"\")  # Clear the result label\n",
        "\n",
        "    def classify_handwriting(self):\n",
        "        #if self.drawing_area.find_all():\n",
        "            #drawn_area = self.drawing_area.bbox(\"all\") # Get the bounding box of all items on the canvas \n",
        "            #x1, y1, x2, y2 = drawn_area # Extract the coordinates of the bounding box\n",
        "            # This helps to only take the Region of Intrest\n",
        "            # So if the user only draws in the top-left corner. It does not affect the output        \n",
        "            HWND = self.drawing_area.winfo_id()  # Get the window ID of the drawing area\n",
        "            rect = win32gui.GetWindowRect(HWND)  # Get the coordinates of the drawing area\n",
        "            withoutLine = 0\n",
        "            rect = (rect[0] + withoutLine, rect[1] + withoutLine, rect[2] - withoutLine, rect[3] - withoutLine)\n",
        "            im = ImageGrab.grab(rect)  # Capture the screen area within the drawing area        \n",
        "            # Preprocess the image for character recognition\n",
        "            #im = im.crop((x1, y1, x2, y2)) # Crop the image to a smaller region defined by a new bounding box\n",
        "            im = im.resize((28, 28))  # Resize image to 28x28 pixels\n",
        "            im = im.convert('L')  # Convert RGB to grayscale\n",
        "            im = np.array(im)  # Convert to NumPy array\n",
        "            im = 255 - im\n",
        "            im = im.astype(np.float32)\n",
        "            im = im.reshape(-1, 28 * 28)  # Reshape to (1, 784)\n",
        "            print(im)\n",
        "        # Convert to PyTorch tensor\n",
        "            img_tensor = torch.tensor(im, dtype=torch.float32)\n",
        "        \n",
        "        # Predicting the class\n",
        "            with torch.no_grad():\n",
        "                outputs = model(img_tensor)  # Forward pass to get model predictions\n",
        "\n",
        "    # Get the top 3 predicted classes and their probabilities\n",
        "                _, top_classes = torch.topk(outputs, 3, dim=1)\n",
        "                top_probabilities = torch.softmax(outputs, dim=1)[0, top_classes].numpy()\n",
        "\n",
        "            # Initialize an empty list to store the top recognized characters and their confidence percentages\n",
        "            top_recognized_chars = []\n",
        "\n",
        "            # Loop through the top 3 predicted classes\n",
        "            for i in range(3):\n",
        "                # Get the class index from the top_classes tensor\n",
        "                class_index = top_classes[0, i].item()\n",
        "                # Check if the class index exists in the class_mapping dictionary\n",
        "                if class_index in class_mapping:\n",
        "                # Retrieve the recognized character for the class index\n",
        "                    recognized_char = class_mapping[class_index]      \n",
        "                    # Get the confidence (probability) for the current prediction\n",
        "                    confidence = top_probabilities[0, i]        \n",
        "                    # Calculate the accuracy percentage by multiplying confidence by 100 and rounding to 2 decimal places\n",
        "                    accuracy_percentage = round(confidence * 100, 2)\n",
        "                    # Append a tuple containing the recognized character and its accuracy percentage to the list\n",
        "                    top_recognized_chars.append((recognized_char, accuracy_percentage))\n",
        "                    # Now, you can update your result_label with the top 3 recognized characters and their percentages\n",
        "\n",
        "                    # Check if there are top recognized characters in the list\n",
        "                if top_recognized_chars:\n",
        "                    # If there are recognized characters, create a formatted result text\n",
        "                    result_text = \"\\n\".join([f\"{char} ({percentage}%)\" for char, percentage in top_recognized_chars])\n",
        "                    # Update the text of the result_label widget with the formatted result text\n",
        "                    self.result_label.configure(text=result_text)\n",
        "                else:\n",
        "                    # If no recognized characters are found, display a message\n",
        "                    self.result_label.configure(text=\"No recognized characters found in the mapping\")\n",
        "\n",
        "# Create an instance of the App class\n",
        "app = App()\n",
        "# Run the GUI application\n",
        "app.mainloop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
